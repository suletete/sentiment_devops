This directory contains the code for the backend of the application, which loads the sentiment analysis model and provides an endpoint through which users can get predictions for their sentences. 
## Start-up the ML service WITH DOCKER
1. Run `docker build <docker_repo_name>/<tag> .` to build the Docker image. The `Dockerfile` contains the instructions for building the Docker image.
	1. __This image is ~3.1GB right now, but needs to be smaller.__
2. Run `docker images` to find the Docker image ID for the image you just built.
3. Run `docker run -d -p 8000:8000 --name=sentiment-anlysis-backend <docker_image_ID>`.
	1. The backend service will start up in the background on [http://0.0.0.0:8000/](http://0.0.0.0:8000/), so you should see "ML model service is running!" when you visit that endpoint. 
	2. The servoce also accomodates HTTP POST requests, which can be sent to [http://0.0.0.0:8000/predict/](http://0.0.0.0:8000//predict/) to get sentiment scores for multiple sentences.
		1. The body of the POST request should look like:
		```
		{
			"sentences":[
						  "this place is the worst!",
            			  "this place is the best!",
            			  "I love this place."
            			]
        }
		```
		2. This can be done using an application such as [Postman](https://www.postman.com/).
3. To ssh into the container, run `docker exec -it <container_name> /bin/bash`. You can `cd` into the `logs/` directory and look at the logs for service as you send requests to it.
4. Stop the container using `docker stop <container_ID>` and remove it using `docker rm <container_ID>`.

## Start-up the ML service WITHOUT DOCKER
1. Use the the `sentiment_analysis_model_env.yaml` file in this directory to create a Conda environment and start it.
	1. Run `conda env create -f sentiment_analysis_model_env.yaml`.
	2. Run `conda activate sentiment_analysis_model_env` to start the Conda environment.
2. Install the project by running `pip install -e .`. This will allow all of the modules for this part of the application to be imported correctly.
3. Test that this backend application can be run properly using `pytest`.
	1. Run `pytest -v` from this directory. This will test the application using the tests contained in the `tests/` directory.
	2. At the bottom of your Terminal screen, you should see `5 passed, 2 warnings`, indicating that the app should work correctly.
	3. The warning are generated by Tensorflow because a `tostring()` method is deprecated, which is not really an issue.
4. Start up the application locally:
	1. From the root directory, execute `chmod +x run.sh`. This shell script will start up a Gunicorn server that runs the Flask application.
	2. From the root directory, execute `./run.sh`.
	3. The service will start up on [http://0.0.0.0:8000/](http://0.0.0.0:8000/), which is the home page. You should see "ML model service is running!" when you visit that endpoint. 
	4. Look at __step 3.2__ from the _Start-up the application WITH DOCKER_ section for sending POST requests to the service.


## Folder Descriptions
1. `Docker_related_files`: Contains files that specify the packages/dependencies needed to run the backend application in a Docker container.
2. `logs`: Contains different _.log_ files that provide information on how a user interacted with the application and any errors that occurred.
	1. The `logging.yaml` file specifies configurations for the logger used by the Flask application.
2. `ml_model_api`: Contains files implementing the backend of the application.
	1. `ml_model_blueprint.py` contains a Flask Blueprint that details which endpoints are served and how they are served.
3. `model_training`: Contains code for training models. It contains a `README` providings details for how this directory is organized.
4. `tests/`: Contains test files for testing the backend application.
5. `utils`: Contains code and functions that are shared. Currently, it only contains a single Python file called `initialize_logging.py`, which starts up the logger with the proper configurations.
